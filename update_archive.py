import requests
from urllib.parse import urlencode
import re
import csv
import pandas as pd
import html


def identify_missing_incomplete():
    """Identify missing and incomplete Rooster Teeth videos from the Internet Archive
    - Missing video URLs: `data/missing.txt`
    - Incomplete upload URLs: `data/incomplete_rt.txt` and `data/incomplete_archive.txt`
    """
    url = "https://archive.org/services/search/v1/scrape"
    query = {
        'q': 'scanner:"Roosterteeth Website Mirror"',
        'fields': 'identifier,addeddate,item_size,format',
        'count': 10000
    }

    archive_items = []
    incomplete = []

    count = 1
    while True:
        response = requests.get(f"{url}?{urlencode(query)}")
        json = response.json()

        for item in json['items']:
            if (
                "roosterteeth-" in item["identifier"] and
                "roosterteeth-test" not in item["identifier"] and
                "-bonus-bonus" not in item["identifier"]
            ):
                archive_items.append(item["identifier"])

                if not (
                    "MPEG4" in item['format'] and
                    "JSON" in item['format'] and # .info.json file
                    "Unknown" in item['format'] and # .description file
                    (
                        "JPEG" in item['format'] or
                        "PNG" in item['format'] or
                        "Animated GIF" in item['format'] or
                        "JPEG 2000" in item['format']
                    )
                ):
                    incomplete.append(item['identifier'])

        if 'cursor' in json:
            query['cursor'] = json['cursor']
        else:
            break

        count += 1

    print(f"Identified {len(archive_items):,} items from the Internet Archive Scrape API across {count:,} requests")

    with open("data/archive_urls.txt", "r") as fp:
        archive_ids = [line.rstrip().replace("https://archive.org/details/", "") for line in fp]

    with open("data/rt_urls.txt", "r") as fp:
        rt_urls = [line.rstrip() for line in fp]

    archive_items = set(archive_items)
    missing = [x for x in archive_ids if x not in archive_items]
    print(f"Found {len(missing):,} items missing from Internet Archive")
    with open("data/missing.txt", "w") as fp:
        for item in missing:
            fp.write(f"{rt_urls[archive_ids.index(item)]}\n")

    print(f"Found {len(incomplete):,} incomplete items on Internet Archive")
    with open("data/incomplete_rt_urls.txt", "w") as fp:
        for item in incomplete:
            fp.write(f"{rt_urls[archive_ids.index(item)]}\n")
    with open("data/incomplete_archive_urls.txt", "w") as fp:
        for item in incomplete:
            fp.write(f"https://archive.org/details/{item}\n")

    # Update README metrics
    with open("README.md", "r") as fp:
        readme = fp.read()
    readme = re.sub(r"(?<=\* Items on Internet Archive: )([\d, \(.\%\)]+)", f"{len(archive_items):,} ({len(archive_items) / len(rt_urls):.2%})", readme)
    readme = re.sub(r"(?<=\* Items Missing from Internet Archive: )([\d, \(.\%\)]+)", f"{len(missing):,} ({len(missing) / len(rt_urls):.2%})", readme)
    readme = re.sub(r"(?<=\* Incomplete Items on Internet Archive: )([\d,]+)", f"{len(incomplete):,}", readme)
    with open("README.md", "w") as f:
        f.write(readme)

    return missing, incomplete


def generate_checklist(missing, incomplete):
    """Generates CSV file for RT Archival Checklist
    * Requires intermediary file (`data/.temp.csv`) generated by `update_rt.py`
    * Writes output to `data/checklist.csv`

    Columns: title, rt_id, rt_url, show, date, is_first, is_uploaded, is_complete_upload, is_removed
    """
    dark = []  # Removed Archive IDs (Updated daily by update_archive_dark.py)
    with open("data/dark.csv", "r") as fp:
        reader = csv.reader(fp)
        next(reader)  # Skip header
        for row in reader:
            dark.append(row[0].replace("https://archive.org/details/", ""))

    with open("data/.temp.csv", "r") as input, open("data/checklist.csv", "w") as output:
        reader = csv.reader(input)
        writer = csv.writer(output)

        checklist = []
        header = next(reader)
        header.extend(['is_uploaded', 'is_complete_upload', 'is_removed'])
        checklist.append(header)

        for row in reader:
            identifier = f"roosterteeth-{row[1]}"
            is_uploaded = identifier not in missing
            is_complete_upload = is_uploaded and identifier not in incomplete
            is_removed = identifier in dark
            row.extend([is_uploaded, is_complete_upload, is_removed])
            checklist.append(row)

        writer.writerows(checklist)


def generate_website():
    """Generates upload progress website"""
    df = pd.read_csv("data/checklist.csv")

    def aggregator(x):
        d = {}
        d['Videos'] = x['rt_id'].count()
        d['Uploaded'] = x['is_uploaded'].sum() + x['is_removed'].sum()
        d['Missing'] = d['Videos'] - d['Uploaded']
        d['Incomplete'] = d['Uploaded'] - x['is_complete_upload'].sum() - x['is_removed'].sum()
        d['Removed'] = x['is_removed'].sum()
        d['Availability'] = f"{(d['Uploaded'] - d['Incomplete'] - d['Removed']) / d['Videos']:.2%}"
        return pd.Series(d, index=list(d.keys()))

    df_shows = df.groupby("show").apply(aggregator, include_groups=False)
    df_shows.sort_index(key=lambda x: x.str.lower(), inplace=True)

    totals = aggregator(df)

    html_table = df_shows.to_html(index_names=False, border=0, bold_rows=False,
                                  table_id="showTable", classes="w3-table-all")
    html_table = html_table.replace("<th></th>", "<th>Show</th>", 1)  # Force proper index header

    # Inject Archive search links
    for title in df_shows.index.to_list():
        query = {
            'query': f'scanner:"Roosterteeth Website Mirror" AND show_title:"{title}"',
            'sort': '-date'
        }
        url = f"https://archive.org/search?{urlencode(query)}"
        title_escaped = html.escape(title, quote=False)
        old = f"<td>{title_escaped}</td>"
        new = f'<td>{title_escaped} <a href="{url}" target="_blank" title="Search on Internet Archive">ðŸ”Ž</a></td>'
        html_table = html_table.replace(old, new)

    last_updated = pd.Timestamp.now(tz="UTC").strftime('%Y-%m-%d %X %Z')

    html_string = f"""
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>Rooster Teeth Website Archive Progress</title>
        <link rel="stylesheet" type="text/css" href="w3.css">
        <style>
            td a {{
                text-decoration: none;
            }}
        </style>
    </head>
    <body>
        <div class="w3-container w3-center">
            <h1>Rooster Teeth Website Archive Progress</h1>
        </div>
        <div class="w3-row w3-center">
            <div class="w3-col s2 w3-blue">
                <h4>Rooster Teeth Videos</h4>
                <h2>{totals['Videos']:,}</h2>
            </div>
            <div class="w3-col s2 w3-green">
                <h4>Uploaded to Archive</h4>
                <h2>{totals['Uploaded']:,}</h2>
            </div>
            <div class="w3-col s2 w3-deep-orange w3-text-white">
                <h4>Missing from Archive</h4>
                <h2>{totals['Missing']:,}</h2>
            </div>
            <div class="w3-col s2 w3-orange w3-text-white">
                <h4>Incomplete Uploads</h4>
                <h2>{totals['Incomplete']:,}</h2>
            </div>
            <div class="w3-col s2 w3-red">
                <h4>Removed Uploads</h4>
                <h2>{totals['Removed']:,}</h2>
            </div>
            <div class="w3-col s2 w3-green">
                <h4>Archive Availability</h4>
                <h2>{totals['Availability']}</h2>
            </div>
        </div>
        <div class="w3-content w3-section">
            <input class="w3-input w3-border w3-padding w3-section" type="text" placeholder="Filter by show name" id="search" onkeyup="search()">
            {html_table}
            <div class="w3-panel w3-border w3-light-grey w3-round-large w3-small">
            <h6>Definitions</h6>
            <ul>
                <li><b>Videos:</b> Number of videos reported by the Rooster Teeth API</li>
                <li><b>Uploaded:</b> Number of videos that have been uploaded to Internet Archive</li>
                <li><b>Missing:</b> Number of videos that have not been uploaded to Internet Archive</li>
                <li><b>Incomplete:</b> Number of Internet Archive uploads without the expected items</li>
                <li><b>Removed:</b> Number of Internet Archive uploads that have been removed from the website</li>
                <li><b>Availability:</b> Percentage of videos that are fully uploaded and available on Internet Archive</li>
            </ul>
            </div>
        </div>
        <div class="w3-container w3-center w3-text-gray">
            <p>Last Updated {last_updated}</p>
        </div>
        <script>
        function search() {{
            var input, filter, table, tr, td, i;
            input = document.getElementById("search");
            filter = input.value.toUpperCase();
            table = document.getElementById("showTable");
            tbody = table.getElementsByTagName("tbody")[0];
            tr = tbody.getElementsByTagName("tr");
            for (i = 0; i < tr.length; i++) {{
                td = tr[i].getElementsByTagName("td")[0];
                if (td) {{
                    txtValue = td.textContent || td.innerText;
                    if (txtValue.toUpperCase().indexOf(filter) > -1) {{
                        tr[i].style.display = "";
                    }} else {{
                        tr[i].style.display = "none";
                    }}
                }}
            }}
        }}
        </script>
    </body>
    </html>
    """

    with open("docs/index.html", "w") as fp:
        fp.write(html_string)


if __name__ == "__main__":
    missing, incomplete = identify_missing_incomplete()
    generate_checklist(missing, incomplete)
    generate_website()
